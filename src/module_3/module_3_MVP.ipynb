{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.metrics import precision_score, precision_recall_curve, recall_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.level=logging.INFO\n",
    "ch = logging.StreamHandler()\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TAKEAWAYS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps required:\n",
    "1. Data loading and validation\n",
    "2. Pre-processing\n",
    "3. Model training/selection\n",
    "- Data is loaded and checked for missing columns, incorrect data types and missing values\n",
    "- Data is preprocessed to filter it, encode some variables and scale\n",
    "- A pipeline is built to join the steps\n",
    "- GridsearchCV is used to tune hiperparameters and select the best model\n",
    "- The best model is saved and stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING AND VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/alvaro/groceries/boxbuilder.csv'\n",
    "\n",
    "required_columns = ['variant_id', 'product_type', 'order_id', 'user_id', 'created_at',\n",
    "       'order_date', 'user_order_seq', 'outcome', 'ordered_before',\n",
    "       'abandoned_before', 'active_snoozed', 'set_as_regular',\n",
    "       'normalised_price', 'discount_pct', 'vendor', 'global_popularity',\n",
    "       'count_adults', 'count_children', 'count_babies', 'count_pets',\n",
    "       'people_ex_baby', 'days_since_purchase_variant_id',\n",
    "       'avg_days_to_buy_variant_id', 'std_days_to_buy_variant_id',\n",
    "       'days_since_purchase_product_type', 'avg_days_to_buy_product_type',\n",
    "       'std_days_to_buy_product_type']\n",
    "\n",
    "required_datatypes = {'variant_id': 'int64',\n",
    " 'product_type': 'O',\n",
    " 'order_id': 'int64',\n",
    " 'user_id': 'int64',\n",
    " 'created_at': 'O',\n",
    " 'order_date': 'O',\n",
    " 'user_order_seq': 'int64',\n",
    " 'outcome': 'float64',\n",
    " 'ordered_before': 'float64',\n",
    " 'abandoned_before': 'float64',\n",
    " 'active_snoozed': 'float64',\n",
    " 'set_as_regular': 'float64',\n",
    " 'normalised_price': 'float64',\n",
    " 'discount_pct': 'float64',\n",
    " 'vendor': 'O',\n",
    " 'global_popularity': 'float64',\n",
    " 'count_adults': 'float64',\n",
    " 'count_children': 'float64',\n",
    " 'count_babies': 'float64',\n",
    " 'count_pets': 'float64',\n",
    " 'people_ex_baby': 'float64',\n",
    " 'days_since_purchase_variant_id': 'float64',\n",
    " 'avg_days_to_buy_variant_id': 'float64',\n",
    " 'std_days_to_buy_variant_id': 'float64',\n",
    " 'days_since_purchase_product_type': 'float64',\n",
    " 'avg_days_to_buy_product_type': 'float64',\n",
    " 'std_days_to_buy_product_type': 'float64'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_validate(path: str):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Check columns\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"Missing required columns: {set(required_columns) - set(df.columns)}\")\n",
    "\n",
    "    # Check nan values\n",
    "    if df.isnull().any().any():\n",
    "        logger.info('Missing values found')\n",
    "\n",
    "    # Check data types\n",
    "    for col,required_type in required_datatypes.items():\n",
    "        datatype = df.dtypes[col]\n",
    "        if required_type != datatype:\n",
    "            raise TypeError(f\"Data type mismatch for column '{col}': Expected '{required_type}', but got '{datatype}'\")\n",
    "\n",
    "    else:\n",
    "        logger.info('Data loaded correctly')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrderFilter(BaseEstimator, TransformerMixin):\n",
    "    ''' filter dataset to only orders with more than 4 products'''\n",
    "    def fit(self, df):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, df):\n",
    "        ids = df[df.outcome == 1].groupby('order_id').variant_id.count() > 4\n",
    "        df_filtered = df[df.order_id.isin(ids[ids == True].index)]\n",
    "        \n",
    "        return df_filtered\n",
    "\n",
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.freq_dict = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_encoded = X.copy()\n",
    "        categorical_cols = ['product_type','vendor']\n",
    "        for col in categorical_cols:\n",
    "            self.freq_dict[col] = X[col].value_counts().to_dict()\n",
    "            X_encoded[col] = X[col].map(self.freq_dict[col]).fillna(0)\n",
    "        return X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame):\n",
    "    # filter orders with more than 4 products\n",
    "    df = OrderFilter().fit_transform(df)        \n",
    "\n",
    "    # prevent info leakage by separating train, validation and test sets by time\n",
    "    df = df.sort_values(by='order_date')\n",
    "    X = df.drop(['variant_id',\"order_id\",\"user_id\",\"created_at\",\"order_date\",'outcome'], axis=1)  \n",
    "    y = df.outcome\n",
    "\n",
    "    # train 0,6 and val+test 0,4\n",
    "    X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.4, shuffle=False)\n",
    "    # validation 0,2, test 0,2\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, shuffle=False)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL OPTIMIZATION AND SAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame, y_val: pd.DataFrame):\n",
    "    pipe = Pipeline(steps=[\n",
    "            ('freq_encoder', FrequencyEncoder()),\n",
    "            ('scaling', StandardScaler()),\n",
    "            ('lr', LogisticRegression())\n",
    "            ])\n",
    "\n",
    "    params = {\n",
    "        'lr__C': [0.0000001, 0.00001, 0.001]\n",
    "    }\n",
    "\n",
    "\n",
    "    grid = GridSearchCV(estimator=pipe, param_grid=params, scoring=['precision','recall'],refit='precision', verbose=True)\n",
    "    grid.fit(X_train,y_train)\n",
    "\n",
    "    score = grid.score(X_val,y_val)\n",
    "    best_model = grid.best_estimator_\n",
    "    best_params = grid.best_params_\n",
    "\n",
    "    return best_model, best_params, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(best_model,best_params):\n",
    "    param = best_params['lr__C']\n",
    "    joblib.dump(best_model, f'lr_c_{param}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df = load_validate(path)\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = preprocess(df)\n",
    "    best_model, best_params, score = select_model(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    logger.info(f'Score: {score}')\n",
    "    logger.info(f'Best hyperparameters: {best_params}')\n",
    "\n",
    "    save_model(best_model,best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data loaded correctly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvaro/.cache/pypoetry/virtualenvs/zrive-ds-HJx0T28e-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/alvaro/.cache/pypoetry/virtualenvs/zrive-ds-HJx0T28e-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/alvaro/.cache/pypoetry/virtualenvs/zrive-ds-HJx0T28e-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/alvaro/.cache/pypoetry/virtualenvs/zrive-ds-HJx0T28e-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/alvaro/.cache/pypoetry/virtualenvs/zrive-ds-HJx0T28e-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Score: 0.6844106463878327\n",
      "Best hyperparameters: {'lr__C': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zrive-ds-HJx0T28e-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
